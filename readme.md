# Codes for Mixup Training for Generative Models to Defend Membership Inference Attacks

## The structure of this repo.

```
│   models.py
│   readme.md
│
├───attack
│   │   convert_ganleaks_to_json.py
│   │   LBFGS_pytorch.py
│   │   logan_atk.py
│   │   mix_comem_attack.py
│   │   mix_comem_attack_ratio.py
│   │   wb_ganleaks.py
│   │   wb_ganleaks_modified.py
│   │
│   ├───firstPrinciple
│   │       attack.py
│   │       attack_against_pargan.py
│   │       convert_ganleaks_to_json.py
│   │       draw_ratio_eval.py
│   │       eval_all.py
│   │       generateScore.py
│   │       split_score.py
│   │       stat_loss.py
│   │       trainShadows.py
│   │       trainShadows_mix.py
│   │       utils.py
│   │
│   └───jsonfile
│       │   base_inLoss.json
│       │   base_inLoss_mu_sigma.json
│       │   base_outLoss.json
│       │   base_outLoss_mu_sigma.json
│       │   base_scoreAll.json
│       │   idx.json
│       │   mask.json
│       │   mix_inLoss.json
│       │   mix_inLoss_mu_sigma.json
│       │   mix_outLoss.json
│       │   mix_outLoss_mu_sigma.json
│       │   mix_scoreAll.json
│       │
│       ├───atk_mix_comem_baseRef
│       │   └───0
│       │           atk_ratio_mix_avg.json
│       │
│       ├───atk_mix_comem_mixRef
│       │   └───0
│       │           atk_ratio_mix_avg.json
│       │
│       ├───atk_ratio
│       │   ├───0
│       │   │       atk_ratio_base.json
│       │   │       atk_ratio_mix.json
│       │   │       atk_ratio_pargan.json
│       │   │       atk_ratio_relaxLoss_changeAllSign.json
│       │   │
│       │   ├───1
│       │   │       atk_ratio_base.json
│       │   │       atk_ratio_mix.json
│       │   │       atk_ratio_relaxLoss_changeAllSign.json
│       │   │
│       │   └───2
│       │           atk_ratio_base.json
│       │           atk_ratio_mix.json
│       │           atk_ratio_relaxLoss_changeAllSign.json
│       │
│       ├───ganleaks
│       │   └───0
│       │           atk_ratio_base.json
│       │           atk_ratio_mix.json
│       │           atk_ratio_pargan.json
│       │           atk_ratio_relaxLoss_changeAllSign.json
│       │
│       └───logan
│           └───0
│                   atkScore_base.json
│                   atkScore_mix.json
│                   atkScore_pargan.json
│                   atkScore_relaxLoss_changeAllSign.json
│
├───datasetExample
│   └───celeba
│       │   list_attr_celeba.txt
│       │
│       └───img_align_celeba
│               000001.jpg
│               000002.jpg
│
├───drawFigures
│       draw_logan_eval.py
│       draw_ratio_eval.py
│
├───trainTargetModel
│       train_gan.py
│       train_gan_mix.py
│       train_pargan.py
│       train_relaxloss.py
│
└───utility
        saveFake.py
        test_downstream.py
        train_downstream.py
        train_downstream_nogen.py
```

## Instruction

`datasetExample` contains a tiny part of CelebA dataset to show the basic structure of the dataset structure we used. 
To run any codes in this repo, you need to rename `datasetExample` by `dataset` and download the full version from the official website of CelebA. 

`trainTargetModel` contains the scripts to train target DNN models (victims). 

`attack` contains the implementation of membership inference attack algorithms considered in this work.
The sub-folder `firstPrinciple` contains the implementation of LiRA attack, a variant from [Membership Inference Attacks From First Principles](https://ieeexplore.ieee.org/document/9833649). 
The procedure to apply the LiRA attack will show in the next section.

`jsonfile` contains some intermediate results during the membership inference attack. Nothing exists inside the folder until the procedure of LiRA begins.

`utility` contains the scripts to train and test the downstream classifier. Additionally, `saveFake.py` is provided to save fake images generated by target models, which is useful when using the package [`pytorch-fid`](https://github.com/mseitzer/pytorch-fid) to calculate Fréchet Inception Distance (FID).

`drawFigures` contains the scripts to visualize privacy result, i.e. the performance of MIA in different settings.

## Procedure
The main process to reproduce this work is:
1. Download the dataset.
2. Train the target model using user-specified protection mode. 
3. Apply the MIA attacks to the target model.
4. Evaluate the performance of MIA attacks in the Step 3.
5. Evaluate the utility of the target model.

Most steps are straight forward using the corresponding scripts, except the LiRA attack which needs a bit more complex steps:
1. Train shadow models using `trainShadows.py` (Or `trainShadows_mix.py` for adaptive LiRA)
2. Generate all scores from reference models and target model to images in test set, using `generateScore.py`
3. Split these scores into two parts: "in loss" and "out loss", using `split_score.py`. 
4. Calculate some statistical features of "in loss" and "out loss" distributions, using `stat_loss.py`. 
5. Run the main code `attack.py` to apply the LiRA attack based on the intermediate results in json format, which are produced by previous steps and stored in the `jsonfile` folder.